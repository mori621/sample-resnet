{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_sample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXEYdvjQAfV4"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZN_C1XzAa1v"
      },
      "source": [
        "## import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zSW3HdQ_49K",
        "outputId": "2cbad309-0f5c-4c9e-c60b-a65b37896de1"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ssrgem3d\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-ssrgem3d\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101634 sha256=5c88746921dd766ceaae0793b697a9dd83a583451fe08647dacd63d580f3e56b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2g_n32ab/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCnfyoDJAiRG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBUwh6vEAlxc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LoyXX51A9IN"
      },
      "source": [
        "from transformers import BertConfig\n",
        "from transformers import BertModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I9BW4iTAsYi"
      },
      "source": [
        "## data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLKFNew6Ar3p",
        "outputId": "ddc9da92-bf10-45f5-d5ac-6d37374da30d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUZjvEieAyx7",
        "outputId": "1fc6827f-a85f-450b-e5fd-b40ca0f2c468"
      },
      "source": [
        "x_train_pd = pd.read_csv('/content/drive/MyDrive/reversi/dataset/train_boards.csv', index_col=0, header=0)\n",
        "x_test_pd = pd.read_csv('/content/drive/MyDrive/reversi/dataset/test_boards.csv', index_col=0, header=0)\n",
        "y_train_pd = pd.read_csv('/content/drive/MyDrive/reversi/dataset/train_labels.csv', index_col=0, header=0)\n",
        "y_test_pd = pd.read_csv('/content/drive/MyDrive/reversi/dataset/test_labels.csv', index_col=0, header=0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTe_i6NMBBho"
      },
      "source": [
        "x_train_list = x_train_pd.values.tolist()\n",
        "x_test_list = x_test_pd.values.tolist()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lg0QdJQBDte"
      },
      "source": [
        "tokenized_x_train = []\n",
        "tokenized_x_test = []\n",
        "\n",
        "for i in x_train_list:\n",
        "  #下駄を履かせる\n",
        "  new_list = [(j+150) for j in i]\n",
        "  tokenized_x_train.append([101] + new_list + [102])\n",
        "\n",
        "for i in x_test_list:\n",
        "  #下駄を履かせる\n",
        "  new_list = [(j+150) for j in i]\n",
        "  tokenized_x_test.append([101] + new_list + [102])\n",
        "\n",
        "#attentionの作成\n",
        "train_attention = torch.ones(len(tokenized_x_train), 66)\n",
        "test_attention = torch.ones(len(tokenized_x_test), 66)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcnK9OQoA3JK"
      },
      "source": [
        "#次元削減\n",
        "y_train = y_train_pd.values.squeeze()\n",
        "y_test = y_test_pd.values.squeeze()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ecTq-85BaR0"
      },
      "source": [
        "#pytorch用のDataset作成\n",
        "class ClassifierDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, x_data, attention, y_data):\n",
        "        self.x_data = x_data\n",
        "        self.attention = attention\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.attention[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.x_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49yZlDu2BhnX"
      },
      "source": [
        "#bertの入力はlongの型しか受け付けないので注意（ベクトルなので）\n",
        "train_dataset = ClassifierDataset(torch.tensor(tokenized_x_train).long(), train_attention.long(), torch.from_numpy(y_train).long())\n",
        "test_dataset = ClassifierDataset(torch.tensor(tokenized_x_test).long(), test_attention.long(), torch.from_numpy(y_test).long())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQdMxk4bBlHe"
      },
      "source": [
        "#DataLoader作成\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz5lOMTWBwYM"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHax-wkWBr5g"
      },
      "source": [
        "#パラメータはbert将棋を参考、要改善\n",
        "#vocab_sizeは入力の種類、hidden_sizeは出力の数\n",
        "\n",
        "config = {\n",
        "    'vocab_size': 152,\n",
        "    'hidden_size': 768,\n",
        "    'num_hidden_layers': 12,\n",
        "    'num_attention_heads': 12,\n",
        "    'intermediate_size': 3072,\n",
        "    'hidden_act': 'gelu',\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'attention_probs_dropout_prob': 0.1,\n",
        "    'max_position_embeddings': 512, \n",
        "    'type_vocab_size': 1, \n",
        "    'initializer_range': 0.02,\n",
        "}\n",
        "\n",
        "config = BertConfig.from_dict(config)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mjgc5XZCSCV"
      },
      "source": [
        "#モデル作成\n",
        "#sigmoidが必要かどうか\n",
        "class BertNextAction(nn.Module):\n",
        "  def __init__(self, model_config):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel(model_config)\n",
        "    self.layer_output = nn.Linear(768, 64)\n",
        "\n",
        "    # self.tanh = nn.Tanh()\n",
        "    # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#labelをどうするか\n",
        "#学習と予測両方が行えるようにlabelsを設定する必要がある\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    x = self.bert(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n",
        "    x = self.layer_output(x).mean(axis=1)\n",
        "    # x = self.sigmoid(x)\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a01VtVhkI0Xt"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOZa0HcfCwAO"
      },
      "source": [
        "model_bert = BertNextAction(config).to(device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVjQiURGC1hb"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_bert.parameters(), lr=0.001)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCVFC13jDDXv"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPLlPWQGC-vi"
      },
      "source": [
        "def train_epoch(model, optimizer, criterion, dataloader, device):\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, (boards, attention, labels) in enumerate(dataloader):\n",
        "\n",
        "        boards, attention, labels = boards.to(device), attention.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(boards, attention)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            # print(f'Epoch [{epoch+1}, {i+1}], train_Loss : {train_loss:.4f}')\n",
        "\n",
        "    train_loss = train_loss / len(dataloader.dataset)\n",
        "    return train_loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG5m5yd7DQn4"
      },
      "source": [
        "def inference(model, optimizer, criterion, dataloader, devide):\n",
        "\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    preds = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for j, (boards, attention, labels) in enumerate(dataloader):\n",
        "\n",
        "            boards, attention, labels = boards.to(device), attention.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(boards, attention)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss = test_loss / len(dataloader.dataset)\n",
        "    return test_loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVJp_u9DDZ_U"
      },
      "source": [
        "def run(num_epochs, model, optimizer, criterion, trainloader, testloader, device):\n",
        "    train_loss_list = []\n",
        "    test_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, optimizer, criterion, trainloader, device)\n",
        "        test_loss = inference(model, optimizer, criterion, testloader, device)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}], train_Loss : {train_loss:.4f}, val_Loss : {test_loss:.4f}')\n",
        "        train_loss_list.append(train_loss)\n",
        "        test_loss_list.append(test_loss)\n",
        "    return train_loss_list, test_loss_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41xClXv7Ff_T"
      },
      "source": [
        "train_loss_list, test_loss_list = run(2, model_bert, optimizer, criterion, trainloader, testloader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAK-XUsPGbQk",
        "outputId": "26706c5e-fc1c-45d4-ae32-e8610c07dcf5"
      },
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    model_bert.train()\n",
        "    for i, (inputs, attention, labels) in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, attention, labels = inputs.to(device), attention.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model_bert(inputs, attention)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 4.154\n",
            "[1,  4000] loss: 4.104\n",
            "[1,  6000] loss: 4.100\n",
            "[1,  8000] loss: 4.098\n",
            "[1, 10000] loss: 4.097\n",
            "[1, 12000] loss: 4.096\n",
            "[1, 14000] loss: 4.096\n",
            "[1, 16000] loss: 4.096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYLCXjfe9Ac"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/reversi/sample_bert.pth')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}